# Copyright 2025 The Protoscribe Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Multimodal PMMX-One flaxformer architecture.

from __gin__ import dynamic_registration

import seqio
from protoscribe.pmmx import feature_converters
from protoscribe.pmmx import pmmx_architecture

from flaxformer.architectures.t5 import t5_architecture
from flaxformer.components import embedding

include "protoscribe/pmmx/configs/architectures/p1_t5_1_1_flaxformer.gin"

# Architecture (Flax Module).
ARCHITECTURE = @pmmx_architecture.MultimodalEncoderDecoder()

# Vocabulary for the encoder.
inputs/PASSTHROUGH_VOCABULARY = @seqio.PassThroughVocabulary()
inputs/seqio.PassThroughVocabulary.size = 0

# Output vocabulary for the decoder. The `SKETCH_TOKEN_VOCAB_SIZE` corresponds
# to the real sketch token vocabulary size (N+3), where 3 is the number of special
# symbols rounded by the batch size B (16): In other words, N + B. This is
#
# - 2064: for N=2048
# - 4112: for N=4096

SKETCH_TOKEN_VOCAB_SIZE = 2064
END_OF_SKETCH = 3 # sketch_tokenizer.END_OF_SKETCH
NUM_EMBEDDINGS = %SKETCH_TOKEN_VOCAB_SIZE

outputs/PASSTHROUGH_VOCABULARY = @seqio.PassThroughVocabulary()
outputs/seqio.PassThroughVocabulary.size = 0
outputs/seqio.PassThroughVocabulary.eos_id = %END_OF_SKETCH

# Actual multimodal encoder-decoder architecture.
pmmx_architecture.MultimodalEncoderDecoder:
  encoder_factory = @pmmx_architecture.MultimodalEncoder
  decoder_factory = @t5_architecture.Decoder
  shared_token_embedder_factory = @token_embedder/embedding.Embed
  dtype = %ACTIVATION_DTYPE

feature_converters.MultimodalEncDecFeatureConverterFactory:
  task_feature_lengths = %TASK_FEATURE_LENGTHS
  feature_specs = (
      ("text.phonetic_embedding", "float32", 2),
  )

# Encoder
pmmx_architecture.MultimodalEncoder:
  feature_spec = [
      ("text.phonetic_embedding", "text.phonetic_embedding"),
  ]
  modality_spec = [
      "text.phonetic_embedding",
  ]
  modality_embedders_spec = {
      "text.phonetic_embedding": [
          ("text.phonetic_embedding", @pmmx_architecture.DenseEmbed)
      ],
  }
